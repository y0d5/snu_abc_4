<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computing System for AI - 강의노트</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Malgun Gothic', '맑은 고딕', sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f0f0f0;
        }
        .container { max-width: 1200px; margin: 0 auto; padding: 20px; }
        header {
            background: #f0f0f0;
            color: #333;
            padding: 30px 20px;
            text-align: center;
            margin-bottom: 24px;
            border-bottom: 2px solid #003366;
        }
        header h1 { font-size: 1.5em; margin-bottom: 8px; color: #003366; font-weight: 600; }
        header .meta { color: #666; font-size: 0.9em; }
        
        /* 슬라이드 섹션: 왼쪽 이미지 + 오른쪽 포인트 */
        .slide-section {
            background: white;
            margin-bottom: 16px;
            border-radius: 10px;
            box-shadow: 0 2px 6px rgba(0,0,0,0.08);
            overflow: hidden;
            display: flex;
            flex-direction: row;
            align-items: stretch;
        }
        .slide-left {
            flex: 0 0 320px;
            background: #f8f9fa;
            padding: 12px;
            display: flex;
            flex-direction: column;
            align-items: center;
            border-right: 1px solid #eee;
        }
        .slide-num {
            font-size: 0.85em;
            color: #888;
            margin-bottom: 8px;
            font-weight: 600;
        }
        .slide-image {
            width: 100%;
            max-width: 300px;
            border: 1px solid #ddd;
            border-radius: 6px;
        }
        .slide-right {
            flex: 1;
            padding: 16px 20px;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }
        .key-points {
            list-style: none;
            padding: 0;
            margin: 0;
        }
        .key-points li {
            padding: 8px 12px;
            margin-bottom: 6px;
            background: #f8f9fa;
            border-left: 3px solid #003366;
            border-radius: 4px;
            font-size: 0.95em;
            line-height: 1.5;
        }
        .key-points li:last-child {
            margin-bottom: 0;
        }
        .no-points {
            color: #999;
            font-style: italic;
            font-size: 0.9em;
        }
        
        /* Q&A 섹션 */
        .qa-section, .takeaways-section {
            background: white;
            padding: 24px;
            margin-bottom: 16px;
            border-radius: 10px;
            box-shadow: 0 2px 6px rgba(0,0,0,0.08);
        }
        .qa-section h2, .takeaways-section h2 {
            color: #003366;
            margin-bottom: 16px;
            padding-bottom: 8px;
            border-bottom: 2px solid #ddd;
            font-size: 1.3em;
        }
        .qa-item {
            margin-bottom: 16px;
            padding-bottom: 16px;
            border-bottom: 1px solid #f0f0f0;
        }
        .qa-item:last-child {
            margin-bottom: 0;
            padding-bottom: 0;
            border-bottom: none;
        }
        .qa-item .question {
            font-weight: bold;
            color: #333;
            margin-bottom: 6px;
        }
        .qa-item .answer {
            padding-left: 16px;
            color: #555;
            border-left: 3px solid #003366;
        }
        
        /* Key Takeaways */
        .takeaways-section ul {
            list-style: none;
            padding: 0;
        }
        .takeaways-section li {
            padding: 10px 14px;
            margin-bottom: 8px;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-radius: 6px;
            border-left: 3px solid #003366;
        }
        .takeaways-section li:last-child {
            margin-bottom: 0;
        }
        
        footer {
            text-align: center;
            padding: 16px;
            color: #999;
            font-size: 0.85em;
        }
        
        /* 슬라이드 클릭 확대 (라이트박스) */
        .slide-image {
            cursor: pointer;
            transition: opacity 0.2s;
        }
        .slide-image:hover {
            opacity: 0.85;
        }
        .lightbox-overlay {
            display: none;
            position: fixed;
            top: 0; left: 0; right: 0; bottom: 0;
            background: rgba(0, 0, 0, 0.8);
            z-index: 9999;
            justify-content: center;
            align-items: center;
            cursor: pointer;
        }
        .lightbox-overlay.active {
            display: flex;
        }
        .lightbox-overlay img {
            max-width: 90vw;
            max-height: 90vh;
            border-radius: 8px;
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.5);
        }
        .lightbox-close {
            position: fixed;
            top: 20px;
            right: 30px;
            color: white;
            font-size: 36px;
            font-weight: bold;
            cursor: pointer;
            z-index: 10000;
            line-height: 1;
        }
        .lightbox-close:hover {
            color: #ccc;
        }
        .lightbox-nav {
            position: fixed;
            top: 50%;
            transform: translateY(-50%);
            color: white;
            font-size: 48px;
            font-weight: bold;
            cursor: pointer;
            z-index: 10000;
            user-select: none;
            padding: 10px;
            line-height: 1;
        }
        .lightbox-nav:hover {
            color: #ccc;
        }
        .lightbox-prev { left: 20px; }
        .lightbox-next { right: 20px; }
        .lightbox-caption {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            color: white;
            font-size: 0.95em;
            z-index: 10000;
            background: rgba(0,0,0,0.5);
            padding: 6px 16px;
            border-radius: 20px;
        }
        
        /* 반응형: 작은 화면에서는 세로 배치 */
        @media (max-width: 768px) {
            .slide-section {
                flex-direction: column;
            }
            .slide-left {
                flex: none;
                border-right: none;
                border-bottom: 1px solid #eee;
            }
        }
    </style>
</head>
<body>
    <!-- 라이트박스 오버레이 -->
    <div class="lightbox-overlay" id="lightbox" onclick="closeLightbox(event)">
        <span class="lightbox-close" onclick="closeLightbox(event)">&times;</span>
        <span class="lightbox-nav lightbox-prev" onclick="navLightbox(event, -1)">&#8249;</span>
        <img id="lightbox-img" src="" alt="">
        <span class="lightbox-nav lightbox-next" onclick="navLightbox(event, 1)">&#8250;</span>
        <div class="lightbox-caption" id="lightbox-caption"></div>
    </div>
    <div class="container">
        <header>
            <h1>Computing System for AI</h1>
            <div class="meta">
                강연자: 이헌준 | 날짜: 2026년 2월 6일
            </div>
        </header>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 1</div>
                <img src="slides/slide_001.jpg" class="slide-image" alt="슬라이드 1" loading="lazy" onclick="openLightbox('slides/slide_001.jpg', 1)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>강의는 AI를 위한 컴퓨팅 시스템 연산기, 메모리 시스템, 통신 시스템, 하드웨어-소프트웨어 협설계 총 4가지 주제로 구성됨</li>                    <li>AI 서비스는 단순한 이미지 분류에서 알파고, 자율주행, 챗GPT 등 복잡한 서비스로 빠르게 발전해옴</li>                    <li>뉴럴 네트워크는 사람의 뇌를 모델로 하여 뉴런과 시냅스의 네트워크 구조를 단순화한 추상화 모델임</li>                    <li>AI 서비스의 복잡성 증가가 컴퓨팅 시스템 설계에 새로운 문제점들을 야기하고 있음</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 2</div>
                <img src="slides/slide_002.jpg" class="slide-image" alt="슬라이드 2" loading="lazy" onclick="openLightbox('slides/slide_002.jpg', 2)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>컴퓨터 아키텍트는 뉴럴 네트워크의 작동 원리보다는 필요한 연산과 하드웨어 설계에 관심을 가진다</li>                    <li>뉴럴 네트워크는 여러 개의 레이어로 구성되며, 각 레이어의 출력이 다음 레이어의 입력으로 순차적으로 전달된다</li>                    <li>뉴럴 네트워크의 각 레이어는 입력값과 가중치(weight)의 곱셈, 덧셈, 그리고 활성화 함수(activation function) 연산으로 구성된다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 3</div>
                <img src="slides/slide_003.jpg" class="slide-image" alt="슬라이드 3" loading="lazy" onclick="openLightbox('slides/slide_003.jpg', 3)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>오늘의 주제는 AI를 위한 컴퓨팅 시스템, 메모리 시스템, 그리고 통신으로 구성됨</li>                    <li>AI 서비스를 위해 컴퓨터가 어떻게 진화하고 있는지 다룰 예정</li>                    <li>극대용량 데이터 요구사항에 대응하기 위한 메모리 시스템의 발전 방향을 설명할 예정</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 4</div>
                <img src="slides/slide_004.jpg" class="slide-image" alt="슬라이드 4" loading="lazy" onclick="openLightbox('slides/slide_004.jpg', 4)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>AI 서비스가 이미지 인식에서 챗봇, 자율주행 등으로 발전하면서 복잡도(complexity)가 증가하고 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 5</div>
                <img src="slides/slide_005.jpg" class="slide-image" alt="슬라이드 5" loading="lazy" onclick="openLightbox('slides/slide_005.jpg', 5)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>뉴럴 네트워크는 여러 개의 레이어로 구성되며, 각 레이어의 출력이 다음 레이어의 입력으로 순차적으로 전달된다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 6</div>
                <img src="slides/slide_006.jpg" class="slide-image" alt="슬라이드 6" loading="lazy" onclick="openLightbox('slides/slide_006.jpg', 6)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>뉴럴 네트워크의 각 레이어는 입력값과 가중치(weight)의 곱셈, 덧셈, 그리고 활성화 함수(activation function) 연산으로 구성된다</li>                    <li>활성화 함수로는 ReLU(0보다 작은 값을 0으로 변환)와 시그모이드 함수 등이 대표적으로 사용된다</li>                    <li>성능 향상을 위해 레이어 개수가 기존 3-4개에서 10여 개 이상으로 약 10배 증가했다</li>                    <li>기존의 단순한 매트릭스 형태의 가중치 구조에서 더 복잡한 구조로 발전하고 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 7</div>
                <img src="slides/slide_007.jpg" class="slide-image" alt="슬라이드 7" loading="lazy" onclick="openLightbox('slides/slide_007.jpg', 7)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>기존 신경망은 벡터-매트릭스 곱셈 기반의 단순한 레이어로 동작했으나, 현재는 컨볼루션 레이어, 어텐션 레이어 등 복잡한 연산을 사용한다</li>                    <li>뉴럴 네트워크는 여러 개의 레이어로 구성되며, 각 레이어는 입력값과 가중치의 곱셈, 합산, 활성화 함수 적용 과정을 거친다</li>                    <li>고등 지능 구현을 위해 신경망의 레이어 수와 각 레이어의 연산 복잡도가 크게 증가하여 더 많은 연산 자원이 필요하다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 8</div>
                <img src="slides/slide_008.jpg" class="slide-image" alt="슬라이드 8" loading="lazy" onclick="openLightbox('slides/slide_008.jpg', 8)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>CPU 성능 정체와 함께 2012년부터 뉴럴 네트워크가 주목받기 시작했다</li>                    <li>애플리케이션은 더욱 복잡해지고 높은 성능을 요구하는데 CPU 성능은 더 이상 향상되지 않는 위기 상황이다</li>                    <li>이러한 상황으로 인해 AI를 위한 컴퓨팅 시스템, 메모리 시스템, 커뮤니케이션 시스템이 필요하게 되었다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 9</div>
                <img src="slides/slide_009.jpg" class="slide-image" alt="슬라이드 9" loading="lazy" onclick="openLightbox('slides/slide_009.jpg', 9)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>컴퓨터 성능 향상을 위한 기존 기술들이 소진되면서 새로운 CPU 개발이 정체되고 있다</li>                    <li>1986년 RISC 아키텍처 도입으로 CPU 성능이 연간 50%씩 향상되는 황금기를 맞았다</li>                    <li>과거에는 무어의 법칙에 따라 CPU 성능이 꾸준히 향상되어 연산량 증가 문제가 없었다</li>                    <li>2003년 이후 반도체 미세화 한계 등으로 인해 CPU 성능 향상 속도가 현저히 둔화되었다</li>                    <li>2011년 이후 CPU 성능 향상 속도가 급격히 둔화되어 2018년 기준 연간 3.5%만 향상되고 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 10</div>
                <img src="slides/slide_010.jpg" class="slide-image" alt="슬라이드 10" loading="lazy" onclick="openLightbox('slides/slide_010.jpg', 10)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>애플리케이션은 더욱 복잡해지고 높은 성능을 요구하는데 CPU 성능은 더 이상 향상되지 않는 위기 상황이다</li>                    <li>CPU는 하나의 명령어를 순차적으로 처리하는 반면, GPU는 그래픽의 여러 픽셀을 동시에 처리하기 위해 병렬 연산에 최적화되어 설계되었다</li>                    <li>GPU는 원래 Graphics Processing Unit의 약자로 그래픽 처리용으로 개발된 하드웨어였다</li>                    <li>2012년경 GPU의 연산 처리 방식이 AI라는 차세대 워크로드에 매우 적합하다는 것이 발견되었다</li>                    <li>NVIDIA는 2008년경 Tesla 아키텍처를 공개하여 그래픽 처리용 새로운 하드웨어를 소개했다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 11</div>
                <img src="slides/slide_011.jpg" class="slide-image" alt="슬라이드 11" loading="lazy" onclick="openLightbox('slides/slide_011.jpg', 11)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>AI 특화 연산을 위해 GPU는 40만 개의 연산기를 집적하여 병렬 처리 능력을 극대화하고 있다</li>                    <li>GPU의 클록 속도는 약 2.4GHz로 0.5나노초마다 40만 개의 연산을 동시에 수행할 수 있다</li>                    <li>CPU는 코어 수의 한계로 인해 GPU 대비 수백 분의 1 수준의 병렬 처리만 가능하다</li>                    <li>GPU가 AI에 효과적인 두 번째 이유는 높은 메모리 대역폭(Higher Memory Bandwidth)을 제공한다는 점이다</li>                    <li>GPU 칩에는 연산기뿐만 아니라 HBM(High Bandwidth Memory) 컨트롤러가 함께 집적되어 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 12</div>
                <img src="slides/slide_012.jpg" class="slide-image" alt="슬라이드 12" loading="lazy" onclick="openLightbox('slides/slide_012.jpg', 12)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>NVIDIA의 젠슨 황이 'AI가 GPU를 찾았다'고 표현할 만큼 GPU가 AI 워크로드에 최적화되어 있다는 것이 발견되었다</li>                    <li>최신 GPU는 스트리밍 멀티프로세서가 4개의 서브 스트리밍 멀티프로세서로 세분화되어 계층이 추가됨</li>                    <li>현재 GPU는 더 많은 병렬성과 AI 특화 방향으로 발전하고 있음</li>                    <li>NVIDIA가 2008년 테슬라 아키텍처 발표 당시에는 AI가 없었지만, 현재 GPU의 가장 큰 캐시카우는 AI 처리</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 13</div>
                <img src="slides/slide_013.jpg" class="slide-image" alt="슬라이드 13" loading="lazy" onclick="openLightbox('slides/slide_013.jpg', 13)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>GPU는 SM(Streaming Multiprocessor) 구조로 되어 있으며, 각 SM 내부에 여러 개의 SP(Stream Processor)가 병렬로 배치되어 있다</li>                    <li>각 SP 옆에는 RF(Register File)이라는 작은 메모리와 명령어/데이터 저장 메모리가 함께 구성되어 있다</li>                    <li>각 SP는 독립적인 연산기로 동작하여 픽셀별 연산을 병렬로 처리하고 결과를 메모리에 저장한다</li>                    <li>GPU는 클럭 사이클마다 다수의 연산을 동시에 처리할 수 있도록 하드웨어가 설계되어 있다</li>                    <li>GPU는 TPC(Texture Processing Cluster) 8개, TPC당 스트리밍 멀티프로세서 2개, 스트리밍 멀티프로세서당 연산기 8개로 구성되어 한 사이클에 128번 연산 가능</li>                    <li>TPC는 GPU 내부에서 연산(SM)과 그래픽 처리(Texture 유닛)를 효율적으로 관리하기 위해 묶어놓은 중간 그룹; 데이터가 연산기(SM)로 들어가기 전, 필요한 자원(텍스처 처리, 스케줄링 등)을 배분하고 관리하는 '중간 관리자' 역할 수행</li>                    <li>CPU는 한 사이클당 하나의 연산만 가능한 것과 대비되어 컴퓨팅 패러다임이 변화</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 14</div>
                <img src="slides/slide_014.jpg" class="slide-image" alt="슬라이드 14" loading="lazy" onclick="openLightbox('slides/slide_014.jpg', 14)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>GPU는 테라바이트/초 수준의 메모리 대역폭과 80GB 용량의 HBM을 제공한다</li>                    <li>다중 GPU가 필요한 첫 번째 이유는 연산량이 너무 많아 단일 GPU의 연산 능력으로 부족하기 때문이다</li>                    <li>다중 GPU가 필요한 두 번째 이유는 AI 모델이 80GB 메모리 용량을 초과하는 경우가 있기 때문이다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 15</div>
                <img src="slides/slide_015.jpg" class="slide-image" alt="슬라이드 15" loading="lazy" onclick="openLightbox('slides/slide_015.jpg', 15)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>NVIDIA GPU에는 AI 연산 전용 텐서 코어(Tensor Core)가 별도로 설계되어 있다</li>                    <li>텐서 코어는 행렬 곱셈 연산만을 전문적으로 수행하는 하드웨어 유닛이다</li>                    <li>하나의 텐서 코어는 8×4×8 크기의 행렬 곱셈을 한 사이클에 처리할 수 있다</li>                    <li>상업용 GPU는 56개의 TPC를 가지며, 각 TPC는 2개의 스트리밍 멀티프로세서로 구성된다</li>                    <li>현재 GPU는 한 사이클에 약 40만 개의 연산을 수행할 수 있어 과거 128개 연산 대비 성능이 크게 향상되었다</li>                    <li>CPU와 GPU는 연산 패러다임이 달라서 각각 잘하는 작업 영역이 다르다</li>                    <li>GPU는 병렬화가 가능한 연산에 특화되어 있고, 순차적 처리가 필요한 데이터베이스 검색 등은 비효율적이다</li>                    <li>텐서 코어가 없어도 매트릭스 연산이 가능했지만, 해당 패턴이 많아 전용 코어를 추가한 것이다</li>                    <li>AI 연산에는 매트릭스 연산뿐만 아니라 벡터 연산도 필요하므로 별도의 연산기가 필요하다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 16</div>
                <img src="slides/slide_016.jpg" class="slide-image" alt="슬라이드 16" loading="lazy" onclick="openLightbox('slides/slide_016.jpg', 16)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>A100 GPU는 HBM을 사용하여 최대 2TB/s의 메모리 대역폭과 80GB 용량을 달성한다</li>                    <li>HBM은 연산기와 DRAM을 묶어서 데이터를 더 빠르게 처리할 수 있게 한다</li>                    <li>HBM(High Bandwidth Memory)은 인터포저 위에 GPU와 함께 3D로 적층된 메모리 구조이다</li>                    <li>CPU는 사용자가 메모리를 따로 구매하여 장착하지만, GPU는 제조 과정에서 메모리가 함께 통합된다</li>                    <li>GPU와 메모리가 물리적으로 가까이 배치되어 데이터 전송 거리가 짧아지고 에너지 효율성이 높아진다</li>                    <li>HBM은 기존 DDR 메모리보다 압도적으로 높은 메모리 대역폭을 제공한다</li>                    <li>높은 메모리 대역폭으로 인해 동일한 시간 내에 더 많은 데이터를 처리할 수 있어 삼성, 하이닉스 같은 메모리 회사들이 성장하고 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 17</div>
                <img src="slides/slide_017.jpg" class="slide-image" alt="슬라이드 17" loading="lazy" onclick="openLightbox('slides/slide_017.jpg', 17)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>GPU를 여러 대 사용할 때 GPU 간 데이터 통신이 필요하며 이에 따른 비용이 발생함</li>                    <li>기존 PCIe 프로토콜을 통한 GPU 간 통신은 항상 CPU가 개입해야 하는 한계가 있음</li>                    <li>NVIDIA가 2017-2018년경 멜라녹스(Mellanox)를 인수한 이유는 여러 GPU를 연결하는 통신 기술 확보를 위함</li>                    <li>GPU 여러 대를 연결하기 위해 NVLink 또는 NVLink 스위치 기술이 개발, NVIDIA가 GPU 직접 통신으로 패러다임을 바꿈</li>                    <li>CPU는 여전히 필요하며 연산 처리 지시, 메일링, 데이터베이스 검색 등의 역할을 담당</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 18</div>
                <img src="slides/slide_018.jpg" class="slide-image" alt="슬라이드 18" loading="lazy" onclick="openLightbox('slides/slide_018.jpg', 18)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>단일 보드 내 GPU 간 통신은 NVLink를 사용하고, 여러 서버 간 GPU 통신을 위해 NVSwitch를 개발했다</li>                    <li>NVSwitch를 통해 서로 다른 서버에 있는 GPU들도 CPU를 거치지 않고 직접 통신이 가능하다</li>                    <li>NVLink와 NVSwitch를 통해 GPU 간 통신 속도가 최대 600GB/s(총 4.8TB/s)까지 달성 가능하다</li>                    <li>AI 서버 설계 시 AI 전용이면 CPU는 최소 사양으로, GPU를 많이 배치하는 것이 효율적이다</li>                    <li>다양한 작업을 처리해야 하는 시스템이면 CPU 고사양이 필요하고, GPU 수량은 전력 고려해 조정해야 한다</li>                    <li>서버 설계는 사용 목적에 따른 CPU와 GPU 간의 트레이드오프 관계를 고려해야 한다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 19</div>
                <img src="slides/slide_019.jpg" class="slide-image" alt="슬라이드 19" loading="lazy" onclick="openLightbox('slides/slide_019.jpg', 19)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>AMD GPU도 성능이 좋지만 NVIDIA가 시장에서 선호되는 이유에 대한 비교 분석이 필요</li>                    <li>NVIDIA H200은 4페타플롭스, AMD MI300X는 2.61페타플롭스의 연산 성능을 가져 NVIDIA가 우위를 보임</li>                    <li>메모리 대역폭은 AMD가 5.3TB/s, NVIDIA가 4.8TB/s로 비슷하며, 메모리 용량은 AMD가 192GB로 NVIDIA의 141GB보다 큼</li>                    <li>가격 면에서 AMD MI300X는 15,000달러, NVIDIA H200은 30,000달러로 AMD가 2배 저렴함</li>                    <li>성능과 가격을 고려하면 AMD가 유리해 보이지만, 실제로는 대부분 NVIDIA GPU를 사용함</li>                    <li>NVIDIA가 시장을 지배하는 이유는 CUDA라는 친숙한 프로그래밍 언어를 사용하는 반면, AMD는 ROCm이라는 생소한 도구를 사용하기 때문</li>                    <li>CPU와 달리 GPU는 벤더별로 독자적인 소프트웨어 에코시스템을 가지고 있다 (NVIDIA는 CUDA, AMD는 ROCm)</li>                    <li>스타트업에서 비용절감을 위해 AMD GPU로 전환하려 하면 개발자들이 새로운 프로그래밍 언어 학습 부담으로 인해 이탈할 수 있다</li>                    <li>메타 같은 빅테크는 높은 급여로 개발자를 유지하며 AMD GPU 전환이 가능하다</li>                    <li>AI 개발에서는 PyTorch, TensorFlow 같은 프레임워크가 상위 레벨에서 프로그래밍을 쉽게 해주지만, 성능 최적화를 위해서는 CUDA나 ROCm 같은 하위 레벨 코딩이 필요하다</li>                    <li>하위 레벨 GPU 프로그래밍은 아직 라이브 코딩(AI 코드 생성)이 잘 되지 않는 영역이다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 20</div>
                <img src="slides/slide_020.jpg" class="slide-image" alt="슬라이드 20" loading="lazy" onclick="openLightbox('slides/slide_020.jpg', 20)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>GPU/하드웨어 선택 시 네트워크 특성을 결정하는 중요 요소로 'Operation per Byte' 개념이 있다</li>                    <li>Operation per Byte는 메모리에서 가져온 단일 바이트 데이터로 수행할 수 있는 연산 횟수를 의미한다</li>                    <li>단순한 매트릭스-벡터 곱셈에서는 각 가중치 데이터가 한 번의 연산에만 사용되어 효율성이 낮다</li>                    <li>배칭(Batching)을 통해 여러 입력을 동시에 처리하면 매트릭스-매트릭스 곱셈으로 변환되어 Operation per Byte가 향상된다</li>                    <li>배칭 시 동일한 가중치 데이터를 여러 번 재사용할 수 있어 메모리 효율성이 크게 개선된다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 21</div>
                <img src="slides/slide_021.jpg" class="slide-image" alt="슬라이드 21" loading="lazy" onclick="openLightbox('slides/slide_021.jpg', 21)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>GPU는 624 테라 연산/초의 성능을 가지지만, HBM 메모리는 2 테라바이트/초의 데이터만 공급 가능하다</li>                    <li>Operation per Byte가 1인 네트워크에서는 메모리 대역폭 제약으로 GPU 연산 능력이 충분히 활용되지 못한다</li>                    <li>메모리 제약 상황에서 GPU는 데이터를 기다리며 유휴 시간이 발생하여 비효율적이다</li>                    <li>배칭을 통해 여러 입력을 병렬 처리하면 Operation per Byte를 증가시킬 수 있다</li>                    <li>Operation per Byte가 2로 증가하면 같은 메모리 대역폭으로 4테라 연산을 수행할 수 있어 GPU 활용률이 향상된다</li>                    <li>네트워크를 312 오퍼레이션/바이트까지 지원하도록 개선하면 GPU가 쉬지 않고 연산할 수 있게 됨</li>                    <li>오퍼레이션/바이트가 증가하면 시스템 처리량이 올라가다가 특정 지점에서 바운드가 걸림</li>                    <li>시스템 성능은 GPU 연산 성능 바운드와 메모리 성능 바운드 구간으로 나뉨</li>                    <li>HBM2E에서 HBM3로 업그레이드하면 더 적은 오퍼레이션/바이트로도 GPU 코어를 완전히 활용할 수 있음</li>                    <li>GPU 연산기와 메모리는 상호작용 관계로, 메모리 성능이 낮으면 연산기를 추가해도 활용할 수 없음</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 22</div>
                <img src="slides/slide_022.jpg" class="slide-image" alt="슬라이드 22" loading="lazy" onclick="openLightbox('slides/slide_022.jpg', 22)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>서머라이제이션 단계에서는 전체 입력 문장을 한 번에 처리하여 병렬 처리가 가능하다</li>                    <li>서머라이제이션 단계는 높은 연산 집약도로 GPU 코어 성능이 중요하고, 제너레이션 단계는 낮은 연산 집약도로 메모리 성능이 중요하다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 23</div>
                <img src="slides/slide_023.jpg" class="slide-image" alt="슬라이드 23" loading="lazy" onclick="openLightbox('slides/slide_023.jpg', 23)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>ICLR 2024에서 마이크로소프트가 발표한 논문에서 GPU 성능 비교 연구를 수행했다</li>                    <li>A100 GPU는 초당 19.5 테라플롭스의 연산 성능을 가지고, H100 GPU는 66.9 테라플롭스로 3.43배 더 높은 연산 성능을 보인다</li>                    <li>메모리 대역폭 측면에서 A100은 2039 GB/s, H100은 3352 GB/s로 H100이 1.64배 더 우수하다</li>                    <li>연산 성능 향상 비율(3.43배)과 메모리 대역폭 향상 비율(1.64배) 간에 불균형이 존재한다</li>                    <li>H100 GPU는 A100 GPU 대비 2.16배 더 비싸지만, 연산 속도는 3.43배, 메모리 속도는 1.64배 향상됨</li>                    <li>연산 집약적 작업(serialization)은 H100에서, 메모리 집약적 작업(generation)은 A100에서 실행하는 것이 비용 효율적임</li>                    <li>무조건 비싼 GPU를 사용하는 것이 아니라 작업의 특성(operation per byte)에 맞는 GPU를 선택해야 함</li>                    <li>젠슨 황의 한국 방문이 NVIDIA의 메모리 확보를 위한 영업 활동이라는 시각이 있음</li>                    <li>GPU와 HBM의 통합 제품 구조로 인해 NVIDIA와 한국 메모리 제조사 간 협상력 관계에 대한 논의</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 24</div>
                <img src="slides/slide_024.jpg" class="slide-image" alt="슬라이드 24" loading="lazy" onclick="openLightbox('slides/slide_024.jpg', 24)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>GPU 외에도 다양한 AI 하드웨어 솔루션들이 등장하고 있으며, 각각 고유한 프로그래밍 방식을 사용한다</li>                    <li>FPGA는 로직을 올리는데 며칠이 걸려 프로그래머빌리티가 떨어지지만, 특정 상황에서는 매우 높은 성능을 제공한다</li>                    <li>ASIC은 특정 작업만을 위해 설계된 칩으로, CPU→GPU→FPGA→ASIC 순으로 효율성과 속도는 증가하지만 유연성과 프로그래머빌리티는 감소한다</li>                    <li>AI 모델이 지속적으로 변화하기 때문에 ASIC 개발에 1-2년이 걸리면, 칩이 출시될 때쯤 AI 생태계가 달라져 성능이 떨어질 수 있다</li>                    <li>이러한 이유로 현재까지는 GPU가 AI 분야에서 널리 사용되고 있다</li>                    <li>하지만 더 이상 근본적으로 새로운 AI 모델이 나오지 않을 가능성도 제기되고 있다</li>                    <li>ASIC 칩 개발업체들이 GPU의 한계를 지적하며 AI 전용 칩의 필요성을 강조하고 있음</li>                    <li>GPU는 학습용, ASIC는 추론용이라는 일반적 인식이 있지만 실제로는 더 복잡한 문제임</li>                    <li>ASIC으로 학습용 칩을 만들면 개발 시간이 1-2년 소요되어 새로운 모델 개발에는 GPU가 더 적합함</li>                    <li>추론 단계에서는 기존 모델을 사용할 수 있어 효율성이 좋은 ASIC 칩 활용 가능성이 있음</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 25</div>
                <img src="slides/slide_025.jpg" class="slide-image" alt="슬라이드 25" loading="lazy" onclick="openLightbox('slides/slide_025.jpg', 25)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>NPU(Neural Processing Unit)는 GPU보다 AI에 특화된 프로세서로, 전력 소모를 20-30% 줄이면서 성능을 유지할 수 있다고 주장된다</li>                    <li>AI 전용 하드웨어는 특정 모델(예: 대형 언어모델)에 특화되어 있어 범용성에 제약이 있다</li>                    <li>구글의 TPU가 하드웨어 스페셜라이제이션의 대표적인 예시이다</li>                    <li>알파고는 GPT 이전의 중요한 AI 혁신 사례로, 강화학습을 통해 바둑에서 인간을 이길 수 있게 되었다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 26</div>
                <img src="slides/slide_026.jpg" class="slide-image" alt="슬라이드 26" loading="lazy" onclick="openLightbox('slides/slide_026.jpg', 26)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>강화학습 기술은 예전부터 존재했지만 이를 실현하기 위한 컴퓨팅 인프라의 부족이 연구의 제약 요인이었다</li>                    <li>구글 DeepMind 팀은 AlphaGo를 위해 전용 TPU(Tensor Processing Unit)를 개발하여 사용했다고 발표했다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 27</div>
                <img src="slides/slide_027.jpg" class="slide-image" alt="슬라이드 27" loading="lazy" onclick="openLightbox('slides/slide_027.jpg', 27)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>TPU는 당시 최신 GPU/CPU 대비 15-30배 빠른 성능과 30-80배 높은 전력 효율성을 보였다(현재는 2배 정도면 우수한 수준)</li>                    <li>AI 전용 특화 하드웨어(Specialized Hardware) 설계의 중요성이 대두되었으며, 매트릭스 곱셈 유닛이 핵심 구성요소였다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 28</div>
                <img src="slides/slide_028.jpg" class="slide-image" alt="슬라이드 28" loading="lazy" onclick="openLightbox('slides/slide_028.jpg', 28)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>구글은 CUDA 대신 TensorFlow라는 새로운 소프트웨어 모델을 도입하여 하드웨어와 소프트웨어를 함께 혁신했다</li>                    <li>하지만 여전히 CUDA 장점이 큼. AI 개발 시 배포 관점과 연구/최적화 관점을 구분해서 접근해야 함</li>                    <li>배포용으로는 기존 프레임워크와 라이브러리가 잘 구축되어 있어 활용 가능</li>                    <li>최신 모델이나 커스텀 환경에서는 기존 라이브러리 지원이 부족해 자체 개발 필요</li>                    <li>CUDA는 소스코드와 자료가 풍부해 커스텀 개발 시 상대적으로 접근성이 좋음</li>                    <li>AI 코딩 지원 도구 사용 시에도 CUDA가 데이터가 많아 더 유리함</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 29</div>
                <img src="slides/slide_029.jpg" class="slide-image" alt="슬라이드 29" loading="lazy" onclick="openLightbox('slides/slide_029.jpg', 29)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>ASIC은 AI 추론용과 학습용 두 가지 목적으로 모두 제작 가능하며, TPU도 추론과 학습 모두에 사용됨</li>                    <li>구글 TPU는 학습용과 추론용을 모두 개발했으나, 일반적으로 ASIC은 추론용이 상업성이 더 높음</li>                    <li>학습용 하드웨어는 이전 모델 기반으로 설계되어 새로운 차세대 모델에는 비효율적일 수 있어 시장성이 제한적</li>                    <li>구글 내부에서는 TPU 강제 사용으로 인해 GPU 대비 개발 허들이 생기는 경우가 있었음</li>                    <li>TPU가 NVIDIA GPU만큼 널리 사용되지 못하는 이유는 구글 생태계에 특화된 환경으로 인한 범용성 부족과 CUDA 같은 개발 도구의 부재</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 30</div>
                <img src="slides/slide_030.jpg" class="slide-image" alt="슬라이드 30" loading="lazy" onclick="openLightbox('slides/slide_030.jpg', 30)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>TPU는 매트릭스 연산을 빠르게 처리할 수 있는 병렬적이고 에너지 효율적인 연산기를 내장하여 성능을 향상시켰다</li>                    <li>시스토닉 어레이라는 효율적인 연산기를 사용하여 매트릭스 곱셈 연산에 특화된 하드웨어 구조를 가지고 있다</li>                    <li>2017년 TPU v1이 당시 GPU보다 성능이 좋았던 이유는 이러한 전용 연산기 때문이었다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 31</div>
                <img src="slides/slide_031.jpg" class="slide-image" alt="슬라이드 31" loading="lazy" onclick="openLightbox('slides/slide_031.jpg', 31)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>TPU v2에서는 HBM 메모리를 선제적으로 도입</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 32</div>
                <img src="slides/slide_032.jpg" class="slide-image" alt="슬라이드 32" loading="lazy" onclick="openLightbox('slides/slide_032.jpg', 32)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>v3에서는 수냉 쿨링 시스템을 도입했다</li>                    <li>현재 데이터센터 구성에서 가장 큰 문제는 쿨링 시스템이며, 냉각 관련 회사들이 많이 등장하고 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 33</div>
                <img src="slides/slide_033.jpg" class="slide-image" alt="슬라이드 33" loading="lazy" onclick="openLightbox('slides/slide_033.jpg', 33)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>국내 AI 가속 칩 설계 회사로 퓨리오사 AI, 리벨리온, 하이퍼 엑셀 등이 있다</li>                    <li>해외에서도 인텔, 애플 등 다양한 회사들이 NPU를 개발하고 있다</li>                    <li>애플의 모바일 칩은 CPU, GPU와 함께 뉴럴 엔진 NPU를 별도로 탑재한다</li>                    <li>저전력이 중요한 모바일 환경에서는 GPU만으로 충분하지 않아 NPU가 필요하다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 34</div>
                <img src="slides/slide_034.jpg" class="slide-image" alt="슬라이드 34" loading="lazy" onclick="openLightbox('slides/slide_034.jpg', 34)">            </div>
            <div class="slide-right">
                <p class="no-points">(내용 없음)</p>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 35</div>
                <img src="slides/slide_035.jpg" class="slide-image" alt="슬라이드 35" loading="lazy" onclick="openLightbox('slides/slide_035.jpg', 35)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>해외에서도 인텔, 애플 등 다양한 회사들이 NPU를 개발하고 있다</li>                    <li>애플의 모바일 칩은 CPU, GPU와 함께 뉴럴 엔진 NPU를 별도로 탑재한다</li>                    <li>저전력이 중요한 모바일 환경에서는 GPU만으로 충분하지 않아 NPU가 필요하다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 36</div>
                <img src="slides/slide_036.jpg" class="slide-image" alt="슬라이드 36" loading="lazy" onclick="openLightbox('slides/slide_036.jpg', 36)">            </div>
            <div class="slide-right">
                <p class="no-points">(내용 없음)</p>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 37</div>
                <img src="slides/slide_037.jpg" class="slide-image" alt="슬라이드 37" loading="lazy" onclick="openLightbox('slides/slide_037.jpg', 37)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>LLM은 서머라이제이션(Prefill) 단계와 제너레이션 단계로 나뉜다</li>                    <li>제너레이션 단계에서는 토큰을 하나씩 순차적으로 생성하며, 이전 출력을 다시 입력으로 사용한다</li>                    <li>루프라인 분석을 통해 각 단계의 시스템 처리량 특성을 분석할 수 있다</li>                    <li>LLM 서비스에서 컨텍스트가 길어질수록 응답 속도가 기하급수적으로 느려지는 문제가 발생한다</li>                    <li>여러 사용자가 모델 파라미터는 공유할 수 있지만, 각자의 대화 컨텍스트는 개별적으로 관리되어야 한다</li>                    <li>LLM에서 입력이 커질수록 KV 캐시가 증가하여 메모리 용량 제한이 병목이 되고 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 38</div>
                <img src="slides/slide_038.jpg" class="slide-image" alt="슬라이드 38" loading="lazy" onclick="openLightbox('slides/slide_038.jpg', 38)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>GPU는 메모리와 연산기가 하나로 결합되어 판매되므로 메모리 용량을 임의로 늘릴 수 없다</li>                    <li>GPU의 HBM 메모리는 80-192GB로 제한적이므로, 초과 데이터는 CPU의 DDR 메모리(2TB 규모)에 저장하는 방식을 사용한다</li>                    <li>자주 사용하는 데이터는 GPU 메모리에, 덜 사용하는 데이터는 CPU 메모리에 저장하여 필요에 따라 데이터를 이동시킨다</li>                    <li>메모리 용량이 더 부족할 경우 SSD까지 활용하는 방안이 논의되고 있다</li>                    <li>NVIDIA의 Vera Rubin 시스템은 GPU와 함께 CPU에 테라바이트 규모의 DDR 메모리를 배치한 아키텍처이다</li>                    <li>HBM 용량이 제한적일 때 데이터를 DDR 메모리와 SSD에 단계적으로 저장하는 메모리 계층 구조를 사용한다</li>                    <li>최근에는 GPU 개수를 늘리는 대신 GPU와 CPU 옆에 DRAM과 SSD를 활용하는 방향으로 패러다임이 변화하고 있다</li>                    <li>이러한 메모리 확장 방식은 추론뿐만 아니라 학습에도 적용 가능하다</li>                    <li>GPU의 높은 비용과 HBM 용량 한계로 인해 대형 모델 처리를 위한 메모리 계층화 연구가 활발히 진행되고 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 39</div>
                <img src="slides/slide_039.jpg" class="slide-image" alt="슬라이드 39" loading="lazy" onclick="openLightbox('slides/slide_039.jpg', 39)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>기술 정의: PCIe를 기반으로 한 차세대 인터페이스로, CPU·GPU·메모리 등의 장치를 하나로 연결하는 고급 통신 규약</li>                    <li>메모리 통합: 서로 다른 장치의 메모리(DDR, HBM, SSD 등)를 하나의 공유 풀로 묶어, 별도의 데이터 복사 없이 로드 명령만으로 즉시 접근 가능</li>                    <li>용량 확장성: 메인보드의 DIMM 슬롯 제약을 넘어 PCIe 슬롯을 통해 DRAM 용량을 테라바이트(TB) 단위로 대폭 확장 가능</li>                    <li>자원 효율성: 구버전 DRAM까지 활용할 수 있어 메모리 운용 효율성 극대화</li>                    <li>산업 동향: 삼성전자, SK하이닉스 등 메모리 벤더가 주도 중이며 엑시콘(Exicon), 파두(Fadu) 등 국내 기업들이 상용화 단계에 진입</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 40</div>
                <img src="slides/slide_040.jpg" class="slide-image" alt="슬라이드 40" loading="lazy" onclick="openLightbox('slides/slide_040.jpg', 40)">            </div>
            <div class="slide-right">
                <p class="no-points">(내용 없음)</p>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 41</div>
                <img src="slides/slide_041.jpg" class="slide-image" alt="슬라이드 41" loading="lazy" onclick="openLightbox('slides/slide_041.jpg', 41)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>DRAM 벤더들이 메모리 내부에 컴퓨팅 유닛을 통합하는 Process-in-Memory 아키텍처를 개발하고 있다</li>                    <li>Process-in-Memory(PIM) 아키텍처는 메모리와 GPU 간의 대역폭 병목을 해결하기 위한 기술이다</li>                    <li>메모리 내부에서 연산을 수행함으로써 메모리와 CPU/GPU 간의 데이터 트래픽을 최소화할 수 있다</li>                    <li>PIM은 연산 로직을 메모리 내부에 구현하여 특정 연산을 메모리에서 직접 처리하고, GPU는 자신이 잘하는 연산만 담당하는 방식이다</li>                    <li>기존 방식에서는 메모리에서 GPU로 데이터를 전송하는 대역폭이 연산기 속도보다 느려 GPU가 유휴 상태가 되는 문제가 있다</li>                    <li>Generation 단계는 메모리 성능에 바운드가 걸리고, Prefill 단계는 GPU 성능에 바운드가 걸린다</li>                    <li>메모리 바운드 연산은 Processing-in-Memory(PIM)에서 처리하는 것이 효율적이다</li>                    <li>GPU 바운드 연산은 PIM에서 처리할 모티베이션이 없고 GPU에서 처리해야 한다</li>                    <li>PIM의 연산기 성능은 메모리 제조 공정으로 만들어지기 때문에 일반 GPU보다 떨어진다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 42</div>
                <img src="slides/slide_042.jpg" class="slide-image" alt="슬라이드 42" loading="lazy" onclick="openLightbox('slides/slide_042.jpg', 42)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>삼성은 PIM을 단독 출시하지 않고 AMD와 협업하여 각각의 장점을 살린 하이브리드 솔루션을 제공한다</li>                    <li>삼성은 HBM에 컴퓨팅 기능을 넣어 AMD GPU와 통합한 클러스터를 만들고 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 43</div>
                <img src="slides/slide_043.jpg" class="slide-image" alt="슬라이드 43" loading="lazy" onclick="openLightbox('slides/slide_043.jpg', 43)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>하이닉스는 AIM이라는 제품을 GDDR로 개발하고 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 44</div>
                <img src="slides/slide_044.jpg" class="slide-image" alt="슬라이드 44" loading="lazy" onclick="openLightbox('slides/slide_044.jpg', 44)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>UPMEM은 PIM 아키텍처를 판매하는 유일한 회사였으나 2025년 퀄컴에 인수됨</li>                    <li>AI가 아닌 데이터베이스 등 다양한 태스크를 타겟으로 하는 PIM 아키텍처 개발</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 45</div>
                <img src="slides/slide_045.jpg" class="slide-image" alt="슬라이드 45" loading="lazy" onclick="openLightbox('slides/slide_045.jpg', 45)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>Cerebras라는 회사는 NVIDIA GPU 대비 매우 큰 사이즈의 칩을 만들어 내부에 SRAM 메모리를 대용량으로 집적하는 방식을 채택했다</li>                    <li>DRAM은 SRAM 대비 동일 용량에서 사이즈가 작지만, SRAM으로 DRAM 수준의 용량을 구현하려면 칩 사이즈가 커져야 한다</li>                    <li>대형 칩 제조 시 디펙트(결함) 발생률이 높아져 수율이 떨어지는 문제가 존재한다</li>                    <li>대형 칩에서 하나의 결함이 발생하면 전체 칩을 폐기해야 하므로 수율 문제로 인한 비용 효율성이 떨어짐</li>                    <li>결함 보정 솔루션을 통해 수율 문제를 해결하는 방법이 개발되어 활용되고 있음</li>                    <li>이러한 방식으로 제작된 칩은 NVIDIA GPU보다 성능이 빠르지만 비용 문제는 여전히 존재함</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 46</div>
                <img src="slides/slide_046.jpg" class="slide-image" alt="슬라이드 46" loading="lazy" onclick="openLightbox('slides/slide_046.jpg', 46)">            </div>
            <div class="slide-right">
                <p class="no-points">(내용 없음)</p>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 47</div>
                <img src="slides/slide_047.jpg" class="slide-image" alt="슬라이드 47" loading="lazy" onclick="openLightbox('slides/slide_047.jpg', 47)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>TPU 시스템은 서버 하나당 64개의 TPU가 4×4×4 구조로 구성되어 총 4096개의 TPU 칩으로 운영된다</li>                    <li>데이터센터 내 원거리 통신에서는 물리적 거리와 저항으로 인해 구리선 통신의 한계가 존재한다</li>                    <li>구글은 서버 내부는 구리선으로, 서버 간 원거리 통신은 옵티컬 링크를 사용하여 문제를 해결한다</li>                    <li>NVIDIA도 실리콘 포토닉스와 옵티컬 인터커넥트 기술 개발에 집중하고 있다</li>                    <li>과거와 달리 현재는 연산량과 메모리 요구사항이 급격히 증가하여 다중 칩 시스템이 필수가 되었다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 48</div>
                <img src="slides/slide_048.jpg" class="slide-image" alt="슬라이드 48" loading="lazy" onclick="openLightbox('slides/slide_048.jpg', 48)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>현대 AI 모델은 단일 GPU로 처리할 수 없어 다중 GPU 시스템(스케일 아웃)이 필요하다</li>                    <li>AI 모델이 커지면서 GPU를 4대, 8대, 16대씩 사용해야 하는데, 메모리 용량 증가 속도가 AI 모델 크기 증가 속도를 따라가지 못함</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 49</div>
                <img src="slides/slide_049.jpg" class="slide-image" alt="슬라이드 49" loading="lazy" onclick="openLightbox('slides/slide_049.jpg', 49)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>모델 크기가 커져서 GPU 메모리에 하나의 모델을 모두 올릴 수 없어 여러 GPU로 분산하여 사용하고 있다</li>                    <li>연산량 증가로 인해 다중 GPU/TPU 시스템을 구성하여 병렬 처리하고 있다</li>                    <li>실제 운영에서는 추론과 학습을 함께 돌리며 데이터센터 간 작업을 분산시키고 있다</li>                    <li>데이터센터에서 습도와 열로 인한 GPU 간 성능 불균형 문제가 발생하여 실시간으로 연산 위치를 조정하는 시스템이 필요하다</li>                    <li>NVIDIA의 NVLink, Arista Networks, Astera Labs 등이 스케일아웃 인터커넥트 인프라를 제공하는 핵심 업체들이다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 50</div>
                <img src="slides/slide_050.jpg" class="slide-image" alt="슬라이드 50" loading="lazy" onclick="openLightbox('slides/slide_050.jpg', 50)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>Google TPU 버전 4 시스템은 서버 보드 8개를 하나의 랙으로 구성하고 인터커넥트로 연결한 구조임</li>                    <li>Scale-Out 인터커넥트 분야의 주요 기업으로 Arista Networks, Astera Labs 등이 있음</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 51</div>
                <img src="slides/slide_051.jpg" class="slide-image" alt="슬라이드 51" loading="lazy" onclick="openLightbox('slides/slide_051.jpg', 51)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>예전 CPU 설계는 하나의 칩에 모든 기능을 집적했지만, 하나의 결함이 발생하면 전체 칩을 폐기해야 하는 문제가 있었다</li>                    <li>AMD 등 현대 회사들은 칩을 여러 개로 분할하여 설계함으로써 일부 결함이 발생해도 나머지 부분을 활용할 수 있게 했다</li>                    <li>칩 분할 방식은 수율이 낮아도 비용 측면에서 더 효율적인 장점을 제공한다</li>                    <li>분할된 칩들 간의 통신 문제가 새로운 기술적 과제로 대두되었다</li>                    <li>데이터센터 내 GPU, TPU 간 통신 최적화는 회로 설계자와 시스템 아키텍처 설계자 모두가 해결해야 할 중요한 문제다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 52</div>
                <img src="slides/slide_052.jpg" class="slide-image" alt="슬라이드 52" loading="lazy" onclick="openLightbox('slides/slide_052.jpg', 52)">            </div>
            <div class="slide-right">
                <p class="no-points">(내용 없음)</p>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 53</div>
                <img src="slides/slide_053.jpg" class="slide-image" alt="슬라이드 53" loading="lazy" onclick="openLightbox('slides/slide_053.jpg', 53)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>소프트웨어-하드웨어 코디자인은 AI 모델을 설계할 때 하드웨어에서의 실행 효율성을 고려하는 접근법이다</li>                    <li>AI 시스템 설계에서 정확도보다 속도(스피드)가 더 중요한 요소일 수 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 54</div>
                <img src="slides/slide_054.jpg" class="slide-image" alt="슬라이드 54" loading="lazy" onclick="openLightbox('slides/slide_054.jpg', 54)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>네트워크 프루닝은 신경망에서 불필요한 가중치(파라미터)를 제거하는 기법이다</li>                    <li>구조화된 프루닝(Structured Pruning)은 뉴런 전체를 제거하여 하드웨어 호환성을 높이지만 압축률이 낮다</li>                    <li>비구조화된 프루닝(Unstructured Pruning)은 개별 연결을 선택적으로 제거하는 방식이다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 55</div>
                <img src="slides/slide_055.jpg" class="slide-image" alt="슬라이드 55" loading="lazy" onclick="openLightbox('slides/slide_055.jpg', 55)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>프루닝 기법은 파라미터 개수와 연산량, 저장 용량을 줄이지만 정확도가 떨어지는 단점이 있다</li>                    <li>NVIDIA A100 GPU의 텐서 코어는 2:4 structural sparsity를 하드웨어적으로 지원한다</li>                    <li>2:4 sparsity는 4개의 연속적인 웨이트 중 2개의 제로 엘레멘트를 제거하여 연산량을 줄이는 기법이다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 56</div>
                <img src="slides/slide_056.jpg" class="slide-image" alt="슬라이드 56" loading="lazy" onclick="openLightbox('slides/slide_056.jpg', 56)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>AI 모델에서 양자화(quantization)는 실수 대신 정수를 사용하여 데이터를 나타내는 기법으로, 뉴럴네트워크에서 정확도를 유지하면서 효율성을 높일 수 있다</li>                    <li>양자화의 장점은 정수 연산이 실수 연산보다 하드웨어 오버헤드가 적고 저장에 필요한 비트 수가 줄어든다는 것이다</li>                    <li>quantization은 학습 시 사용하는 정밀한 floating point 값을 배포 시 정수로 변환하는 기법이다</li>                    <li>신경망의 양자화(Quantization)는 저정밀도 값을 사용하여 메모리와 연산량을 4배까지 줄일 수 있다</li>                    <li>Dense 연산과 Sparse 연산을 비교했을 때 Sparse 연산이 2배의 성능 향상을 보인다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 57</div>
                <img src="slides/slide_057.jpg" class="slide-image" alt="슬라이드 57" loading="lazy" onclick="openLightbox('slides/slide_057.jpg', 57)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>GPU에서 FP32(32비트)에서 FP16(16비트)으로 정밀도를 낮추면 연산 성능이 크게 향상된다</li>                    <li>FP16에서 FP8로 더 낮은 정밀도로 갈수록 연산 성능이 계속 증가한다</li>                    <li>비트 수를 줄일수록 연산 성능은 향상되지만 품질 유지를 위한 튜닝이 중요하다</li>                    <li>양자화와 프루닝 같은 알고리즘 최적화 기법을 통해 AI 모델의 성능을 지속적으로 향상시킬 수 있다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 58</div>
                <img src="slides/slide_058.jpg" class="slide-image" alt="슬라이드 58" loading="lazy" onclick="openLightbox('slides/slide_058.jpg', 58)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>Agentic AI는 기존의 단순한 입력-출력 구조와 달리 반복적이고 상호작용적인 추론 과정을 거쳐 시스템 복잡도가 크게 증가했다</li>                    <li>과거에는 프리필과 디코딩만으로 충분했지만, 현재는 AI가 생성한 출력을 바탕으로 모델이 여러 번 반복 실행되어 연산량이 크게 늘어났다</li>                    <li>Agentic AI는 중간에 데이터베이스 검색, 웹 검색, API 호출 등 다양한 액션을 수행해야 하는 복잡한 워크플로우를 가진다</li>                    <li>데이터베이스 검색이나 웹 검색 같은 작업들은 GPU를 활용하지 않아서 비싼 GPU 자원이 유휴 상태가 되는 문제가 발생한다</li>                    <li>최근 AI 트렌드가 생성형 AI에서 에이전틱 AI(Agentic AI)로 전환되고 있다</li>                    <li>Chain of Thought에서는 중간 추론 과정을 생성하고 이를 다시 입력으로 사용하여 최종 답안을 도출한다</li>                </ul>            </div>
        </div>

        <div class="slide-section">
            <div class="slide-left">
                <div class="slide-num">슬라이드 59</div>
                <img src="slides/slide_059.jpg" class="slide-image" alt="슬라이드 59" loading="lazy" onclick="openLightbox('slides/slide_059.jpg', 59)">            </div>
            <div class="slide-right">
                <ul class="key-points">                    <li>AI 모델 실행 시 GPU와 CPU가 번갈아 사용되면서 한쪽이 유휴 상태가 되는 리소스 비효율성 문제</li>                    <li>복잡한 AI 워크로드에서는 단일 워크로드가 아닌 이중적(heterogeneous) 워크로드를 처리해야 함</li>                    <li>AI 시스템에서 어떤 작업을 실행할지가 비결정적(non-deterministic)이어서 시스템 구성의 최적화가 어려움</li>                    <li>CPU, SSD, GPU 중 어느 것에 투자할지 상황에 따라 달라져 시스템 최적화 전략 수립이 복잡함</li>                    <li>학계에서도 이러한 AI 시스템 최적화 문제가 최근 연구 주제로 부상하고 있음(HPCA 2026 논문)</li>                </ul>            </div>
        </div>

        <div class="takeaways-section">
            <h2>📌 Key Takeaways</h2>
            <ul>
                <li>AI 워크로드의 복잡성 증가로 CPU 성능 향상이 정체된 상황에서 GPU의 병렬 연산 능력(40만 개 연산기)과 높은 메모리 대역폭(HBM)이 AI 컴퓨팅의 핵심 솔루션으로 부상했다</li>                <li>Operation per Byte 개념을 통해 AI 워크로드 특성을 분석하면, 메모리 집약적 작업(Generation)과 연산 집약적 작업(Prefill)에 따라 최적의 하드웨어를 선택할 수 있어 비용 효율성을 극대화할 수 있다</li>                <li>NVIDIA가 GPU 시장을 지배하는 이유는 단순히 하드웨어 성능이 아니라 CUDA라는 친숙한 프로그래밍 생태계와 NVLink 같은 통합 솔루션을 통한 락인 전략 때문이다</li>                <li>AI 모델의 대형화로 인한 메모리 용량 한계를 해결하기 위해 GPU-CPU-SSD 계층화된 메모리 구조와 CXL, PIM 같은 새로운 메모리 기술이 중요해지고 있다</li>                <li>에이전틱 AI로의 전환으로 AI 워크로드가 GPU와 CPU를 번갈아 사용하는 복잡한 패턴으로 변화하여, 단순한 하드웨어 성능보다는 시스템 전체의 최적화가 더욱 중요해졌다</li>            </ul>
        </div>

    <script>
        var slideImages = [];
        var currentIdx = 0;
        document.querySelectorAll('.slide-image').forEach(function(img) {
            slideImages.push({ src: img.getAttribute('src'), alt: img.getAttribute('alt') });
        });

        function openLightbox(src, slideNum) {
            currentIdx = slideImages.findIndex(function(s) { return s.src === src; });
            if (currentIdx < 0) currentIdx = 0;
            showSlide();
            document.getElementById('lightbox').classList.add('active');
            document.body.style.overflow = 'hidden';
        }
        function closeLightbox(e) {
            if (e.target.id === 'lightbox' || e.target.classList.contains('lightbox-close')) {
                document.getElementById('lightbox').classList.remove('active');
                document.body.style.overflow = '';
            }
        }
        function navLightbox(e, dir) {
            e.stopPropagation();
            currentIdx = (currentIdx + dir + slideImages.length) % slideImages.length;
            showSlide();
        }
        function showSlide() {
            var s = slideImages[currentIdx];
            document.getElementById('lightbox-img').src = s.src;
            document.getElementById('lightbox-caption').textContent = s.alt + ' (' + (currentIdx + 1) + '/' + slideImages.length + ')';
        }
        document.addEventListener('keydown', function(e) {
            var lb = document.getElementById('lightbox');
            if (!lb.classList.contains('active')) return;
            if (e.key === 'Escape') { lb.classList.remove('active'); document.body.style.overflow = ''; }
            if (e.key === 'ArrowLeft') { currentIdx = (currentIdx - 1 + slideImages.length) % slideImages.length; showSlide(); }
            if (e.key === 'ArrowRight') { currentIdx = (currentIdx + 1) % slideImages.length; showSlide(); }
        });
    </script>

        <footer>
            생성일시: 2026-02-16 09:14:20 | 자동 생성된 강의노트
        </footer>
    </div>
</body>
</html>
