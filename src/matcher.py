#!/usr/bin/env python3
"""
ìŠ¬ë¼ì´ë“œ-STT ë§¤ì¹­ ëª¨ë“ˆ

[v2] ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹:
  - STTë¥¼ 10ë¶„ ë‹¨ìœ„ ì²­í¬ë¡œ ë¶„í• 
  - ê° ì²­í¬ë¥¼ ìŠ¬ë¼ì´ë“œ ìœˆë„ìš°(~15ì¥)ì™€ LLMìœ¼ë¡œ ë§¤ì¹­
  - ì´ì „ ì²­í¬ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœˆë„ìš°ë¥¼ ì´ë™

[v1] ë ˆê±°ì‹œ ë°©ì‹ (ê¸°ì¡´ ë°ì´í„° í˜¸í™˜):
  - ì‹œê°„ ê¸°ë°˜ ê· ë“± ë¶„í•  + LLM ê²€ì¦
"""

import sys
sys.stdout.reconfigure(line_buffering=True)

import os
import re
import json
from pathlib import Path
from dataclasses import dataclass, field
import anthropic


def load_env_file():
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
    env_path = Path(__file__).parent.parent / ".env"
    if env_path.exists():
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# .env íŒŒì¼ ë¡œë“œ
load_env_file()


@dataclass
class SlideMatch:
    """ìŠ¬ë¼ì´ë“œ-ë°œí™” ë§¤ì¹­ ê²°ê³¼"""
    slide_num: int
    slide_text: str
    utterances: list[dict] = field(default_factory=list)
    confidence: str = "unknown"  # high, medium, low, unknown
    llm_verified: bool = False
    notes: str = ""


def time_based_matching(
    slides_info: list[dict],
    stt_data: dict,
    total_duration_seconds: int
) -> list[SlideMatch]:
    """
    1ì°¨: í˜ì´ì§€ ë²ˆí˜¸ ìš°ì„ , ì—†ìœ¼ë©´ ì‹œê°„ ê¸°ë°˜ ê· ë“± ë¶„í•  ë§¤ì¹­

    - STTì— slide_num(í˜ì´ì§€ ë²ˆí˜¸)ì´ ìˆìœ¼ë©´ í•´ë‹¹ ìŠ¬ë¼ì´ë“œì— ì§ì ‘ ë°°ì • (ìš°ì„ )
    - ì—†ìœ¼ë©´ ê¸°ì¡´ì²˜ëŸ¼ ì‹œê°„ëŒ€ ê¸°ë°˜ìœ¼ë¡œ ë°°ì •
    """
    print("\n[3-1] ìŠ¬ë¼ì´ë“œ-STT ë§¤ì¹­ (í˜ì´ì§€ ë²ˆí˜¸ ìš°ì„ , ì‹œê°„ ê¸°ë°˜ ë³´ì¡°)")
    
    num_slides = len(slides_info)
    utterances = stt_data.get("utterances", [])
    
    if not utterances:
        print("   âš ï¸ ë°œí™” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆëŠ” ë°œí™” ìˆ˜
    page_annotated = sum(1 for u in utterances if u.get("slide_num"))
    if page_annotated:
        print(f"   ğŸ“Œ í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆëŠ” ë°œí™”: {page_annotated}ê°œ (ìš°ì„  ë§¤ì¹­)")
    
    # ë§ˆì§€ë§‰ ë°œí™” ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ (ì‹œê°„ ê¸°ë°˜ ë§¤ì¹­ìš©)
    last_utterance_time = max(u["seconds"] for u in utterances)
    time_per_slide = last_utterance_time / num_slides if num_slides else 1
    
    print(f"   ì´ ìŠ¬ë¼ì´ë“œ: {num_slides}ê°œ")
    print(f"   ì´ ë°œí™”: {len(utterances)}ê°œ")
    print(f"   ë§ˆì§€ë§‰ ë°œí™” ì‹œê°„: {last_utterance_time}ì´ˆ")
    
    # ìŠ¬ë¼ì´ë“œë³„ ë§¤ì¹­ ì´ˆê¸°í™”
    matches = []
    for i, slide in enumerate(slides_info):
        match = SlideMatch(
            slide_num=slide["page_num"],
            slide_text=slide.get("text_preview", ""),
            utterances=[],
            confidence="unknown"
        )
        matches.append(match)
    
    # ê° ë°œí™”ë¥¼ ìŠ¬ë¼ì´ë“œì— ë°°ì •
    for utterance in utterances:
        slide_idx = None
        slide_num = utterance.get("slide_num")
        
        # 1) í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ìŠ¬ë¼ì´ë“œì— ë°°ì • (1-based â†’ 0-based)
        if slide_num is not None and 1 <= slide_num <= num_slides:
            slide_idx = slide_num - 1
        # 2) ì—†ìœ¼ë©´ ì‹œê°„ ê¸°ë°˜ ë°°ì •
        if slide_idx is None:
            seconds = utterance["seconds"]
            slide_idx = min(int(seconds / time_per_slide), num_slides - 1) if num_slides else 0
        
        matches[slide_idx].utterances.append(utterance)
    
    # í†µê³„ ì¶œë ¥
    empty_slides = sum(1 for m in matches if len(m.utterances) == 0)
    print(f"   â†’ ë§¤ì¹­ ì™„ë£Œ: {num_slides - empty_slides}ê°œ ìŠ¬ë¼ì´ë“œì— ë°œí™” ë°°ì •")
    print(f"   â†’ ë°œí™” ì—†ëŠ” ìŠ¬ë¼ì´ë“œ: {empty_slides}ê°œ")
    
    return matches


def llm_verify_matches(
    matches: list[SlideMatch],
    batch_size: int = 5
) -> list[SlideMatch]:
    """
    2ì°¨: LLMì„ ì‚¬ìš©í•˜ì—¬ ë§¤ì¹­ ê²€ì¦ ë° ì¡°ì •
    
    ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ì™€ ë°°ì •ëœ ë°œí™” ë‚´ìš©ì„ ë¹„êµí•˜ì—¬ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸
    """
    print("\n[3-2] LLM ê²€ì¦ ì‹œì‘")
    
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key or api_key == "your_api_key_here":
        print("   âš ï¸ ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   â†’ .env íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        print("   â†’ LLM ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return matches
    
    client = anthropic.Anthropic(api_key=api_key)
    
    # ë°œí™”ê°€ ìˆëŠ” ìŠ¬ë¼ì´ë“œë§Œ ê²€ì¦
    slides_to_verify = [m for m in matches if m.utterances]
    total = len(slides_to_verify)
    
    print(f"   ê²€ì¦ ëŒ€ìƒ: {total}ê°œ ìŠ¬ë¼ì´ë“œ")
    
    verified_count = 0
    adjusted_count = 0
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for i in range(0, total, batch_size):
        batch = slides_to_verify[i:i+batch_size]
        
        for match in batch:
            try:
                result = verify_single_match(client, match, matches)
                if result["verified"]:
                    match.confidence = "high"
                    match.llm_verified = True
                    verified_count += 1
                else:
                    match.confidence = result.get("confidence", "low")
                    match.notes = result.get("notes", "")
                    if result.get("adjusted"):
                        adjusted_count += 1
            except Exception as e:
                print(f"   âš ï¸ ìŠ¬ë¼ì´ë“œ {match.slide_num} ê²€ì¦ ì‹¤íŒ¨: {e}")
                match.confidence = "unknown"
        
        print(f"   â†’ {min(i + batch_size, total)}/{total} ê²€ì¦ ì™„ë£Œ")
    
    print(f"   âœ… ê²€ì¦ ì™„ë£Œ: {verified_count}ê°œ ì¼ì¹˜, {adjusted_count}ê°œ ì¡°ì •ë¨")
    
    return matches


def verify_single_match(
    client: anthropic.Anthropic,
    match: SlideMatch,
    all_matches: list[SlideMatch]
) -> dict:
    """ë‹¨ì¼ ìŠ¬ë¼ì´ë“œ-ë°œí™” ë§¤ì¹­ ê²€ì¦"""
    
    # ë°œí™” ë‚´ìš© ìš”ì•½ (í˜ì´ì§€ ë²ˆí˜¸ íŒíŠ¸ í¬í•¨)
    utterance_lines = []
    page_hint_utterances = []
    for u in match.utterances[:5]:  # ìµœëŒ€ 5ê°œë§Œ
        line = f"[{u['timestamp']}] {u['speaker']}: {u['content'][:200]}"
        utterance_lines.append(line)
        if u.get("slide_num"):
            page_hint_utterances.append(f"  - {u['timestamp']} ë°œí™”: ì›ë³¸ STTì—ì„œ ìŠ¬ë¼ì´ë“œ {u['slide_num']}ë²ˆìœ¼ë¡œ í‘œì‹œë¨")
    utterance_text = "\n".join(utterance_lines)
    page_hint_block = ""
    if page_hint_utterances:
        page_hint_block = "\n## STT í˜ì´ì§€ ë²ˆí˜¸ ì •ë³´ (ìš°ì„  ê³ ë ¤):\n" + "\n".join(page_hint_utterances) + "\n"
    
    # ì¸ì ‘ ìŠ¬ë¼ì´ë“œ ì •ë³´
    slide_idx = match.slide_num - 1
    prev_slide_text = all_matches[slide_idx - 1].slide_text if slide_idx > 0 else ""
    next_slide_text = all_matches[slide_idx + 1].slide_text if slide_idx < len(all_matches) - 1 else ""
    
    prompt = f"""ë‹¹ì‹ ì€ ê°•ì˜ ìŠ¬ë¼ì´ë“œì™€ ê°•ì—° ë‚´ìš©ì„ ë§¤ì¹­í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

í˜„ì¬ ìŠ¬ë¼ì´ë“œ {match.slide_num}ë²ˆì— ë‹¤ìŒ ë°œí™”ë“¤ì´ ë°°ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.{page_hint_block}

## ìŠ¬ë¼ì´ë“œ {match.slide_num} í…ìŠ¤íŠ¸:
{match.slide_text[:500] if match.slide_text else "(í…ìŠ¤íŠ¸ ì—†ìŒ)"}

## ë°°ì •ëœ ë°œí™” ë‚´ìš©:
{utterance_text}

## ì´ì „ ìŠ¬ë¼ì´ë“œ ({match.slide_num - 1}ë²ˆ) í…ìŠ¤íŠ¸:
{prev_slide_text[:200] if prev_slide_text else "(ì—†ìŒ)"}

## ë‹¤ìŒ ìŠ¬ë¼ì´ë“œ ({match.slide_num + 1}ë²ˆ) í…ìŠ¤íŠ¸:
{next_slide_text[:200] if next_slide_text else "(ì—†ìŒ)"}
{"\n**STTì— í˜ì´ì§€ ë²ˆí˜¸ê°€ í‘œì‹œëœ ë°œí™”ëŠ” ì›ë³¸ ë…¹ìŒì—ì„œ í•´ë‹¹ ìŠ¬ë¼ì´ë“œë¡œ ê¸°ë¡ëœ ê²ƒì´ë¯€ë¡œ, ê·¸ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‹ ë¢°í•´ì£¼ì„¸ìš”.**" if page_hint_utterances else ""}

ì´ ë°œí™”ë“¤ì´ í˜„ì¬ ìŠ¬ë¼ì´ë“œì— ì ì ˆí•˜ê²Œ ë§¤ì¹­ë˜ì—ˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
{{
  "match_quality": "good" | "partial" | "poor",
  "confidence": "high" | "medium" | "low",
  "reasoning": "íŒë‹¨ ì´ìœ  (í•œ ë¬¸ì¥)",
  "suggested_slide": í˜„ì¬ ìŠ¬ë¼ì´ë“œê°€ ì ì ˆí•˜ë©´ {match.slide_num}, ì•„ë‹ˆë©´ ë” ì ì ˆí•œ ìŠ¬ë¼ì´ë“œ ë²ˆí˜¸
}}"""

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=500,
        messages=[{"role": "user", "content": prompt}]
    )
    
    # ì‘ë‹µ íŒŒì‹±
    response_text = response.content[0].text
    
    try:
        # JSON ì¶”ì¶œ
        import re
        json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            
            verified = result.get("match_quality") in ["good", "partial"]
            return {
                "verified": verified,
                "confidence": result.get("confidence", "medium"),
                "notes": result.get("reasoning", ""),
                "adjusted": result.get("suggested_slide") != match.slide_num
            }
    except json.JSONDecodeError:
        pass
    
    return {"verified": False, "confidence": "unknown", "notes": "íŒŒì‹± ì‹¤íŒ¨"}


def refine_uncertain_matches(matches: list[SlideMatch]) -> list[SlideMatch]:
    """
    3ì°¨: ë¶ˆí™•ì‹¤í•œ ë§¤ì¹­ ì¬ì¡°ì •
    
    confidenceê°€ lowì¸ ë§¤ì¹­ë“¤ì„ ì¸ì ‘ ìŠ¬ë¼ì´ë“œì™€ ë¹„êµí•˜ì—¬ ì¬ì¡°ì •
    """
    print("\n[3-3] ë¶ˆí™•ì‹¤í•œ ë§¤ì¹­ ì¬ì¡°ì •")
    
    uncertain = [m for m in matches if m.confidence == "low"]
    
    if not uncertain:
        print("   â†’ ì¬ì¡°ì •ì´ í•„ìš”í•œ ìŠ¬ë¼ì´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return matches
    
    print(f"   ì¬ì¡°ì • ëŒ€ìƒ: {len(uncertain)}ê°œ ìŠ¬ë¼ì´ë“œ")
    
    # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ë°œí™” ë‚´ìš©ì— ìŠ¬ë¼ì´ë“œ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    adjusted_count = 0
    
    for match in uncertain:
        # í˜„ì¬ ìŠ¬ë¼ì´ë“œì˜ í‚¤ì›Œë“œ
        current_keywords = set(match.slide_text.lower().split()) if match.slide_text else set()
        
        # ë°œí™” ë‚´ìš©ì˜ í‚¤ì›Œë“œ
        utterance_text = " ".join([u["content"] for u in match.utterances])
        utterance_keywords = set(utterance_text.lower().split())
        
        # í‚¤ì›Œë“œ ê²¹ì¹¨ ì •ë„ ê³„ì‚°
        if current_keywords:
            overlap = len(current_keywords & utterance_keywords) / len(current_keywords)
            if overlap > 0.1:  # 10% ì´ìƒ ê²¹ì¹˜ë©´ ì ì ˆí•œ ê²ƒìœ¼ë¡œ íŒë‹¨
                match.confidence = "medium"
                match.notes += " (í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì¡°ì •)"
                adjusted_count += 1
    
    print(f"   âœ… {adjusted_count}ê°œ ìŠ¬ë¼ì´ë“œ confidence ì¡°ì •ë¨")
    
    return matches


###############################################################################
# v2: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë§¤ì¹­ (ìƒˆ ë°©ì‹)
###############################################################################

DEFAULT_LECTURE_MINUTES = 150   # ê¸°ë³¸ ê°•ì˜ ì‹œê°„ (ë¶„)
CHUNK_MINUTES = 10             # ì²­í¬ ë‹¨ìœ„ (ë¶„)
WINDOW_MULTIPLIER = 3          # ìœˆë„ìš° í¬ê¸° = ì²­í¬ë‹¹ í‰ê·  ìŠ¬ë¼ì´ë“œ Ã— ì´ ë°°ìˆ˜
OVERLAP_BACK = 2               # ìœˆë„ìš° ì‹œì‘ ì‹œ ë’¤ë¡œ ê²¹ì¹˜ëŠ” ìŠ¬ë¼ì´ë“œ ìˆ˜


def sliding_window_matching(
    slides_info: list[dict],
    stt_data: dict,
    total_duration_seconds: int
) -> list[SlideMatch]:
    """
    ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ì˜ ìŠ¬ë¼ì´ë“œ-STT ë§¤ì¹­

    1) ê°•ì˜ ì‹œê°„ ê²°ì • (ê¸°ë³¸ 150ë¶„, STTì™€ 20% ì´ìƒ ì°¨ì´ì‹œ STT ê¸°ì¤€)
    2) ìŠ¬ë¼ì´ë“œë‹¹ í‰ê·  ì‹œê°„ ê³„ì‚°
    3) STTë¥¼ 10ë¶„ ì²­í¬ë¡œ ë¶„í• 
    4) ê° ì²­í¬ë¥¼ ìŠ¬ë¼ì´ë“œ ìœˆë„ìš°ì™€ LLMìœ¼ë¡œ ë§¤ì¹­
    5) ìœˆë„ìš°ë¥¼ ì´ë™í•˜ë©° ë°˜ë³µ
    """
    print("\n[3-1] ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë§¤ì¹­ ì‹œì‘")

    num_slides = len(slides_info)
    utterances = stt_data.get("utterances", [])

    if not utterances or not num_slides:
        print("   âš ï¸ ìŠ¬ë¼ì´ë“œ ë˜ëŠ” ë°œí™” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # --- ê°•ì˜ ì‹œê°„ ê²°ì • ---
    default_seconds = DEFAULT_LECTURE_MINUTES * 60
    stt_last_second = max(u["seconds"] for u in utterances)

    if total_duration_seconds > 0 and abs(total_duration_seconds - default_seconds) / default_seconds < 0.2:
        lecture_seconds = default_seconds
    elif total_duration_seconds > 0:
        lecture_seconds = total_duration_seconds
    else:
        lecture_seconds = stt_last_second if stt_last_second > 0 else default_seconds

    if stt_last_second > lecture_seconds:
        lecture_seconds = stt_last_second

    lecture_minutes = lecture_seconds / 60
    avg_sec_per_slide = lecture_seconds / num_slides
    avg_min_per_slide = avg_sec_per_slide / 60

    print(f"   ğŸ“Š ê°•ì˜ ì‹œê°„: {lecture_minutes:.0f}ë¶„")
    print(f"   ğŸ“Š ìŠ¬ë¼ì´ë“œ: {num_slides}ì¥")
    print(f"   ğŸ“Š ìŠ¬ë¼ì´ë“œë‹¹ í‰ê· : {avg_min_per_slide:.1f}ë¶„")
    print(f"   ğŸ“Š ì´ ë°œí™”: {len(utterances)}ê°œ")

    # --- ì²­í¬ ë¶„í•  ---
    chunk_seconds = CHUNK_MINUTES * 60
    slides_per_chunk = int(CHUNK_MINUTES / avg_min_per_slide) if avg_min_per_slide > 0 else 5
    window_size = max(slides_per_chunk * WINDOW_MULTIPLIER, 10)

    chunks = split_utterances_into_chunks(utterances, chunk_seconds)
    print(f"   ğŸ“Š {CHUNK_MINUTES}ë¶„ ì²­í¬: {len(chunks)}ê°œ, ìœˆë„ìš° í¬ê¸°: {window_size}ì¥")

    # --- ìŠ¬ë¼ì´ë“œ ë§¤ì¹­ ê²°ê³¼ ì´ˆê¸°í™” ---
    matches = []
    for slide in slides_info:
        matches.append(SlideMatch(
            slide_num=slide["page_num"],
            slide_text=slide.get("text_preview", ""),
            utterances=[],
            confidence="unknown"
        ))

    # --- API í´ë¼ì´ì–¸íŠ¸ ---
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key or api_key == "your_api_key_here":
        print("   âš ï¸ ANTHROPIC_API_KEY ì—†ìŒ â†’ ì‹œê°„ ê¸°ë°˜ í´ë°±")
        return _fallback_time_based(matches, utterances, num_slides, stt_last_second)

    client = anthropic.Anthropic(api_key=api_key)

    # --- ì²­í¬ë³„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë§¤ì¹­ ---
    window_start = 0  # 0-based slide index

    for chunk_idx, chunk in enumerate(chunks):
        chunk_start_min = chunk["start_sec"] / 60
        chunk_end_min = chunk["end_sec"] / 60
        chunk_utterances = chunk["utterances"]

        if not chunk_utterances:
            continue

        # ìœˆë„ìš° ë²”ìœ„: ë’¤ë¡œ OVERLAP_BACKë§Œí¼ ì—¬ìœ , ì•ìœ¼ë¡œ window_size
        win_start = max(0, window_start - OVERLAP_BACK)
        win_end = min(num_slides, window_start + window_size)

        # ë§ˆì§€ë§‰ ì²­í¬ë¼ë©´ ë‚¨ì€ ìŠ¬ë¼ì´ë“œ ì „ë¶€ í¬í•¨
        if chunk_idx == len(chunks) - 1:
            win_end = num_slides

        window_slides = slides_info[win_start:win_end]

        print(f"\n   ğŸ”„ ì²­í¬ {chunk_idx + 1}/{len(chunks)} "
              f"({chunk_start_min:.0f}~{chunk_end_min:.0f}ë¶„) "
              f"â†’ ìŠ¬ë¼ì´ë“œ {win_start + 1}~{win_end}ë²ˆ")

        # LLMìœ¼ë¡œ ì²­í¬ ë§¤ì¹­
        try:
            chunk_result = llm_match_chunk(
                client, chunk_utterances, window_slides,
                win_start, chunk_start_min, chunk_end_min
            )
        except Exception as e:
            print(f"      âš ï¸ LLM ë§¤ì¹­ ì‹¤íŒ¨: {e} â†’ ê· ë“± ë¶„í•  í´ë°±")
            chunk_result = _fallback_chunk(chunk_utterances, win_start, win_end)

        # ê²°ê³¼ ë°˜ì˜
        last_matched_idx = win_start
        for mapping in chunk_result:
            slide_idx = mapping["slide_idx"]
            mapped_utterances = mapping["utterances"]

            if 0 <= slide_idx < num_slides:
                matches[slide_idx].utterances.extend(mapped_utterances)
                matches[slide_idx].confidence = mapping.get("confidence", "medium")
                matches[slide_idx].llm_verified = True
                if slide_idx > last_matched_idx:
                    last_matched_idx = slide_idx

        # ë‹¤ìŒ ìœˆë„ìš° ì‹œì‘ì 
        window_start = last_matched_idx + 1
        if window_start >= num_slides:
            window_start = num_slides - 1

        print(f"      â†’ ë§¤ì¹­ ì™„ë£Œ, ë‹¤ìŒ ìœˆë„ìš° ì‹œì‘: ìŠ¬ë¼ì´ë“œ {window_start + 1}ë²ˆ")

    # í†µê³„
    filled = sum(1 for m in matches if m.utterances)
    print(f"\n   âœ… ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë§¤ì¹­ ì™„ë£Œ: {filled}/{num_slides} ìŠ¬ë¼ì´ë“œì— ë°œí™” ë°°ì •")

    return matches


def split_utterances_into_chunks(
    utterances: list[dict],
    chunk_seconds: int
) -> list[dict]:
    """ë°œí™”ë¥¼ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ë¡œ ë¶„í• """
    if not utterances:
        return []

    chunks = []
    current_chunk = {"start_sec": 0, "end_sec": chunk_seconds, "utterances": []}

    for u in utterances:
        sec = u.get("seconds", 0)
        while sec >= current_chunk["end_sec"]:
            chunks.append(current_chunk)
            new_start = current_chunk["end_sec"]
            current_chunk = {
                "start_sec": new_start,
                "end_sec": new_start + chunk_seconds,
                "utterances": []
            }
        current_chunk["utterances"].append(u)

    if current_chunk["utterances"]:
        chunks.append(current_chunk)

    return chunks


def llm_match_chunk(
    client: anthropic.Anthropic,
    chunk_utterances: list[dict],
    window_slides: list[dict],
    window_offset: int,
    chunk_start_min: float,
    chunk_end_min: float
) -> list[dict]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ 10ë¶„ ì²­í¬ì˜ ë°œí™”ë¥¼ ìŠ¬ë¼ì´ë“œ ìœˆë„ìš°ì— ë§¤ì¹­

    Returns:
        list of {"slide_idx": int (0-based global), "utterances": [...], "confidence": str}
    """

    # ë°œí™” ë‚´ìš© êµ¬ì„± (í˜ì´ì§€ ë²ˆí˜¸ íŒíŠ¸ í¬í•¨)
    utterance_lines = []
    page_hints = []
    for i, u in enumerate(chunk_utterances):
        ts = u.get("timestamp", "")
        speaker = u.get("speaker", "")
        content = u.get("content", "")[:300]
        utterance_lines.append(f"[{ts}] {speaker}: {content}")
        if u.get("slide_num"):
            page_hints.append(f"  - [{ts}] ë°œí™”ê°€ STTì—ì„œ ìŠ¬ë¼ì´ë“œ {u['slide_num']}ë²ˆìœ¼ë¡œ í‘œì‹œë¨")
        if len(utterance_lines) >= 30:
            utterance_lines.append(f"... (ì™¸ {len(chunk_utterances) - 30}ê°œ ë°œí™”)")
            break

    utterance_text = "\n".join(utterance_lines)
    page_hint_text = ""
    if page_hints:
        page_hint_text = "\n## STT í˜ì´ì§€ ë²ˆí˜¸ íŒíŠ¸ (ìš°ì„  ê³ ë ¤):\n" + "\n".join(page_hints) + "\n"

    # ìŠ¬ë¼ì´ë“œ ìœˆë„ìš° êµ¬ì„±
    slide_lines = []
    for s in window_slides:
        pnum = s["page_num"]
        text = s.get("text_preview", "")[:300]
        slide_lines.append(f"### ìŠ¬ë¼ì´ë“œ {pnum}ë²ˆ\n{text if text else '(í…ìŠ¤íŠ¸ ì—†ìŒ)'}")

    slides_text = "\n\n".join(slide_lines)

    first_slide = window_slides[0]["page_num"]
    last_slide = window_slides[-1]["page_num"]

    prompt = f"""ë‹¹ì‹ ì€ ê°•ì˜ ìŠ¬ë¼ì´ë“œì™€ ê°•ì—° ë…¹ì·¨ë¡(STT)ì„ ë§¤ì¹­í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ê°•ì˜ì˜ **{chunk_start_min:.0f}ë¶„ ~ {chunk_end_min:.0f}ë¶„** êµ¬ê°„ ë°œí™” ë‚´ìš©ì…ë‹ˆë‹¤.
ì´ ë°œí™”ë“¤ì´ ìŠ¬ë¼ì´ë“œ {first_slide}ë²ˆ ~ {last_slide}ë²ˆ ì¤‘ ì–´ë””ì— í•´ë‹¹í•˜ëŠ”ì§€ ë§¤ì¹­í•´ì£¼ì„¸ìš”.
{page_hint_text}
## ë°œí™” ë‚´ìš© ({chunk_start_min:.0f}~{chunk_end_min:.0f}ë¶„):
{utterance_text}

## ìŠ¬ë¼ì´ë“œ ìœˆë„ìš° ({first_slide}~{last_slide}ë²ˆ):
{slides_text}

## ë§¤ì¹­ ê·œì¹™:
1. ë°œí™” ë‚´ìš©ê³¼ ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ì˜ ì£¼ì œ/í‚¤ì›Œë“œë¥¼ ë¹„êµí•˜ì—¬ ë§¤ì¹­
2. STTì— í˜ì´ì§€ ë²ˆí˜¸ê°€ í‘œì‹œëœ ê²½ìš° í•´ë‹¹ ì •ë³´ë¥¼ ìš°ì„  ì‹ ë¢°
3. ê°•ì˜ëŠ” ìˆœì„œëŒ€ë¡œ ì§„í–‰ë˜ë¯€ë¡œ, ìŠ¬ë¼ì´ë“œ ë²ˆí˜¸ëŠ” ëŒ€ì²´ë¡œ ì˜¤ë¦„ì°¨ìˆœ
4. í•˜ë‚˜ì˜ ë°œí™”ëŠ” í•˜ë‚˜ì˜ ìŠ¬ë¼ì´ë“œì—ë§Œ ë°°ì •
5. ì´ ì‹œê°„ëŒ€ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ìŠ¬ë¼ì´ë“œëŠ” ë¹ˆ ë°°ì—´ë¡œ

JSON ë°°ì—´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì´ ì‹œê°„ëŒ€ì—ì„œ ì‹¤ì œë¡œ ë‹¤ë£¬ ìŠ¬ë¼ì´ë“œë§Œ í¬í•¨:
[
  {{
    "slide_num": ìŠ¬ë¼ì´ë“œ ë²ˆí˜¸,
    "utterance_indices": [ì´ ìŠ¬ë¼ì´ë“œì— í•´ë‹¹í•˜ëŠ” ë°œí™”ì˜ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)],
    "confidence": "high" | "medium" | "low"
  }},
  ...
]"""

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}]
    )

    response_text = response.content[0].text

    # JSON ë°°ì—´ íŒŒì‹±
    try:
        json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
        if json_match:
            result_list = json.loads(json_match.group())
        else:
            print("      âš ï¸ JSON ë°°ì—´ íŒŒì‹± ì‹¤íŒ¨")
            return _fallback_chunk(chunk_utterances, window_offset, window_offset + len(window_slides))
    except json.JSONDecodeError as e:
        print(f"      âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return _fallback_chunk(chunk_utterances, window_offset, window_offset + len(window_slides))

    # ê²°ê³¼ ë³€í™˜
    mapped = []
    for item in result_list:
        slide_num = item.get("slide_num", 0)
        indices = item.get("utterance_indices", [])
        confidence = item.get("confidence", "medium")

        slide_idx = slide_num - 1  # 1-based â†’ 0-based
        selected_utterances = []
        for idx in indices:
            if 0 <= idx < len(chunk_utterances):
                selected_utterances.append(chunk_utterances[idx])

        if selected_utterances:
            mapped.append({
                "slide_idx": slide_idx,
                "utterances": selected_utterances,
                "confidence": confidence
            })

    return mapped


def _fallback_time_based(
    matches: list[SlideMatch],
    utterances: list[dict],
    num_slides: int,
    stt_last_second: float
) -> list[SlideMatch]:
    """API í‚¤ ì—†ì„ ë•Œ ì‹œê°„ ê¸°ë°˜ ê· ë“± ë¶„í•  í´ë°±"""
    time_per_slide = stt_last_second / num_slides if num_slides else 1
    for u in utterances:
        sec = u.get("seconds", 0)
        idx = min(int(sec / time_per_slide), num_slides - 1)
        matches[idx].utterances.append(u)
    return matches


def _fallback_chunk(
    chunk_utterances: list[dict],
    win_start: int,
    win_end: int
) -> list[dict]:
    """LLM ì‹¤íŒ¨ ì‹œ ì²­í¬ ë‚´ ë°œí™”ë¥¼ ìœˆë„ìš°ì— ê· ë“± ë¶„í• """
    window_size = win_end - win_start
    if window_size <= 0:
        return []

    result = []
    per_slide = max(1, len(chunk_utterances) // window_size)

    for i in range(window_size):
        start = i * per_slide
        end = start + per_slide if i < window_size - 1 else len(chunk_utterances)
        selected = chunk_utterances[start:end]
        if selected:
            result.append({
                "slide_idx": win_start + i,
                "utterances": selected,
                "confidence": "low"
            })

    return result


###############################################################################
# ë©”ì¸ ì‹¤í–‰
###############################################################################

def run_matching(output_dir: Path) -> list[SlideMatch]:
    """
    ì „ì²´ ë§¤ì¹­ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (v2: ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)
    """
    print("\n" + "=" * 70)
    print("ğŸ”„ Step 3: ìŠ¬ë¼ì´ë“œ-STT ë§¤ì¹­ ì‹œì‘ (ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)")
    print("=" * 70)
    
    # ë°ì´í„° ë¡œë“œ
    slides_info_path = output_dir / "slides_info.json"
    stt_path = output_dir / "stt_parsed.json"
    metadata_path = output_dir / "metadata.json"
    
    with open(slides_info_path, 'r', encoding='utf-8') as f:
        slides_info = json.load(f)
    
    with open(stt_path, 'r', encoding='utf-8') as f:
        stt_data = json.load(f)
    
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    # STT duration íŒŒì‹± (ì˜ˆ: "94ë¶„ 3ì´ˆ")
    duration_str = metadata.get("stt_duration", "0ë¶„")
    mins = re.search(r'(\d+)ë¶„', duration_str)
    secs = re.search(r'(\d+)ì´ˆ', duration_str)
    total_seconds = (int(mins.group(1)) * 60 if mins else 0) + (int(secs.group(1)) if secs else 0)
    
    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë§¤ì¹­ (v2)
    matches = sliding_window_matching(slides_info, stt_data, total_seconds)
    
    # ê²°ê³¼ ì €ì¥
    result_path = output_dir / "slide_matches.json"
    result_data = [
        {
            "slide_num": m.slide_num,
            "utterance_count": len(m.utterances),
            "confidence": m.confidence,
            "llm_verified": m.llm_verified,
            "notes": m.notes,
            "utterances": m.utterances
        }
        for m in matches
    ]
    
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)
    
    print(f"\n   âœ… ë§¤ì¹­ ê²°ê³¼ ì €ì¥: {result_path.name}")
    
    # í†µê³„
    high_conf = sum(1 for m in matches if m.confidence == "high")
    medium_conf = sum(1 for m in matches if m.confidence == "medium")
    low_conf = sum(1 for m in matches if m.confidence == "low")
    filled = sum(1 for m in matches if m.utterances)
    
    print("\n" + "-" * 70)
    print("ğŸ“Š ë§¤ì¹­ ê²°ê³¼ ìš”ì•½")
    print("-" * 70)
    print(f"   ë°œí™” ë°°ì • ìŠ¬ë¼ì´ë“œ:     {filled}/{len(matches)}ê°œ")
    print(f"   ë†’ì€ ì‹ ë¢°ë„ (high):   {high_conf}ê°œ")
    print(f"   ì¤‘ê°„ ì‹ ë¢°ë„ (medium): {medium_conf}ê°œ")
    print(f"   ë‚®ì€ ì‹ ë¢°ë„ (low):    {low_conf}ê°œ")
    print("=" * 70)
    
    return matches


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        output_dir = Path(sys.argv[1])
        run_matching(output_dir)
    else:
        print("Usage: python matcher.py <output_dir>")
